\section{Introduction}
\textit{Authors: Igor Molcean, Julian Weigel (Hyperledger-specific parts)}

Blockchain is a relatively new concept. It represents a distributed, uncentralized and immutable data storage where every single change can be seen by anyone and the whole amount of data is stored at every node that participates in the network. This makes blockchain quite different from traditional databases that try to restrict access as far as possible and require centralized coordinators for replication. Unlike many distributed storages, blockchain does not use redundancy to increase performance but rather to improve resilience of the system. For the first time, the term was used in context of Bitcoin in 2008 \cite{bitcoin}. This became the reason why blockchain is often associated with cryptocurrencies but the potencial of this technology is actually much higher.

One of the big problems that prevents blockchain-based technologies from entering our everyday life is the limited throughput. For example, limited block size in case of Bitcoin, caused the so-called "Bitcoin scalability problem" which resulted in intense research \cite{bitcoin_scaling} and numerous proposals on how to increase throughput of the system. Being able to perform operations fast, is important for a system that is used by a big number of people worldwide. This is why throughput becomes so critical in context of blockchain systems.

The goal of our experiment is to measure the throughput for Ethereum and Hyperledger, which are two widely used blockchain systems. This is done in order to learn how effective their underlying algorithms process big numbers of submitted transactions in short periods of time.
